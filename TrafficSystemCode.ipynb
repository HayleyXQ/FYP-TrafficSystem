{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daba50a",
   "metadata": {},
   "source": [
    "Traffic Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556d1d8",
   "metadata": {},
   "source": [
    "Data Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049023fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Initial Imports --------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "read_data = 'C:/Users/Ning Sheng Yong/Desktop/QING APU/futuristic_city_traffic.csv'\n",
    "data = pd.read_csv(read_data)\n",
    "\n",
    "# Basic Information\n",
    "print(\"Basic Information:\")\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nData Types:\\n\", data.dtypes)\n",
    "print(\"\\nSummary Statistics:\\n\", data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "print(\"\\nMissing Values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60013684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values in Categorical Columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "print(\"\\nUnique Values in Categorical Columns:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(data[col].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec5da0",
   "metadata": {},
   "source": [
    "Data Visualization (Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic density distribution\n",
    "sns.histplot(data['Traffic Density'], kde=True)\n",
    "plt.title(\"Traffic Density Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle speed vs traffic density\n",
    "sns.scatterplot(x='Speed', y='Traffic Density', hue='Vehicle Type', data=data)\n",
    "plt.title(\"Speed vs Traffic Density by Vehicle Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9434ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of energy consumption\n",
    "sns.boxplot(x='Vehicle Type', y='Energy Consumption', data=data)\n",
    "plt.title(\"Energy Consumption by Vehicle Type\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edf16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "corr = data.select_dtypes(include=np.number).corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33640dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Analysis\n",
    "numeric_data = data.select_dtypes(include=[np.number]).columns\n",
    "grouped_data = data.groupby('City')[numeric_data].mean()\n",
    "print(\"\\nGrouped Analysis by City:\\n\", grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot showing numerical data distribution\n",
    "numericals = [\"Speed\", \"Energy Consumption\", \"Traffic Density\"]\n",
    "sns.pairplot(data, vars=numericals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "for col in numerical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=data[col])\n",
    "    plt.title(f'Boxplot of {col} for Outlier Detection')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Set up grid: 2 plots per row\n",
    "n_cols = 2\n",
    "n_rows = (len(numerical_columns) + n_cols - 1) // n_cols  # ceiling division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 4))\n",
    "\n",
    "# Flatten axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_columns):\n",
    "    sns.boxplot(data=data[col], ax=axes[idx])\n",
    "    axes[idx].set_title(f'Boxplot of {col}')\n",
    "    \n",
    "# Hide any unused subplots\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be2651",
   "metadata": {},
   "source": [
    "Data Pre-processing (Initial - from IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613c17b",
   "metadata": {},
   "source": [
    "Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Outliers\n",
    "# Using IQR to detect outliers\n",
    "numerical_features = data.select_dtypes(include=[np.number]).columns\n",
    "for column in numerical_features:\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Count outliers\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    print(f\"Outliers detected in {column}: {len(outliers)}\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ca98",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize 'Day Of Week' with an actual calendar\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Map Day Of Week strings to numerical values (Monday=0, ..., Sunday=6)\n",
    "day_of_week_mapping = {\n",
    "    \"Monday\": 0,\n",
    "    \"Tuesday\": 1,\n",
    "    \"Wednesday\": 2,\n",
    "    \"Thursday\": 3,\n",
    "    \"Friday\": 4,\n",
    "    \"Saturday\": 5,\n",
    "    \"Sunday\": 6,\n",
    "}\n",
    "\n",
    "# Convert 'Day Of Week' to numerical values using the mapping\n",
    "data[\"Day Of Week\"] = data[\"Day Of Week\"].map(day_of_week_mapping)\n",
    "\n",
    "# Choose an arbitrary starting Monday for syncing the calendar\n",
    "start_date = datetime(2024, 1, 1)  # Example: Start from the first Monday of 2024\n",
    "\n",
    "# Generate synthetic calendar dates based on 'Day Of Week'\n",
    "data[\"Date\"] = data[\"Day Of Week\"].apply(lambda dow: start_date + timedelta(days=dow))\n",
    "#data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "\n",
    "\n",
    "# Validate and display the updated dataset\n",
    "print(data[[\"Day Of Week\", \"Date\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb201230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Public Holiday Feature --------------------\n",
    "manual_holidays = [\n",
    "    datetime(2024, 1, 1), datetime(2024, 2, 10), datetime(2024, 5, 1),\n",
    "    datetime(2024, 8, 31), datetime(2024, 12, 25),\n",
    "    datetime(2025, 1, 1), datetime(2025, 1, 29)\n",
    "]\n",
    "holiday_dates = [d.date() for d in manual_holidays]\n",
    "data['Is_Public_Holiday'] = data['Date'].apply(\n",
    "    lambda x: 1 if pd.notnull(x) and x.date() in holiday_dates else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "# Convert Date/Time to usable features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['Is_Weekend'] = data['Day Of Week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "def categorize_hour(hour):\n",
    "    if 5 <= hour < 12: return \"Morning\"\n",
    "    elif 12 <= hour < 17: return \"Afternoon\"\n",
    "    elif 17 <= hour < 21: return \"Evening\"\n",
    "    else: return \"Night\"\n",
    "\n",
    "data[\"Time of Day\"] = data[\"Hour Of Day\"].apply(categorize_hour)\n",
    "\n",
    "data['Weather_Category'] = data['Weather'].apply(\n",
    "    lambda x: 'Clear' if 'clear' in x.lower()\n",
    "    else 'Rainy' if 'rain' in x.lower()\n",
    "    else 'Snowy' if 'snow' in x.lower()\n",
    "    else 'Other'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f600ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize traffic density\n",
    "def categorize_density(density):\n",
    "    if density >= 0.75: return 'very high'\n",
    "    elif density >= 0.35: return 'high'\n",
    "    elif density >= 0.20: return 'medium'\n",
    "    elif density >= 0.05: return 'low'\n",
    "    else: return 'very low'\n",
    "\n",
    "\n",
    "# Apply the categorization function\n",
    "data['Traffic Density Category'] = data['Traffic Density'].apply(categorize_density)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Speed-Traffic Impact --------------------\n",
    "def determine_speed_impact(row):\n",
    "    speed = row['Speed']\n",
    "    density = row['Traffic Density']\n",
    "    if speed >= 70 and density <= 0.2:\n",
    "        return 'Free-flowing'\n",
    "    elif speed <= 30 and density >= 0.7:\n",
    "        return 'Highly Congested'\n",
    "    elif 30 < speed < 70 and 0.2 < density < 0.7:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Irregular'\n",
    "\n",
    "data['Speed_Traffic_Impact'] = data.apply(determine_speed_impact, axis=1)\n",
    "impact_encoder = LabelEncoder()\n",
    "data['Speed_Traffic_Impact_Label'] = impact_encoder.fit_transform(data['Speed_Traffic_Impact'])\n",
    "\n",
    "print(\"\\nSpeed-Traffic Impact Distribution:\")\n",
    "print(data['Speed_Traffic_Impact'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8551e99",
   "metadata": {},
   "source": [
    "Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be433812",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0: 15]\n",
    "X\n",
    "\n",
    "y = data['Traffic Density Category']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of values in the 'density' column (which contains 'low', 'medium', 'high')\n",
    "target_class_distribution = data['Traffic Density Category'].value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(target_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4dfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the Preprocessed Data\n",
    "print(\"\\nDataset after Preprocessing:\")\n",
    "\n",
    "# Save the cleaned and preprocessed dataset\n",
    "cleaned_file_path = 'C:/Users/Ning Sheng Yong/Desktop/QING APU/cleaned_urban_traffic_density.csv'\n",
    "data.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"Cleaned dataset saved to {cleaned_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397625",
   "metadata": {},
   "source": [
    "Data Preprocessing (Further preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165635ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# ------------------ STEP 0: Load Dataset ------------------\n",
    "read_data = 'C:/Users/Ning Sheng Yong/Desktop/QING APU/cleaned_urban_traffic_density.csv'\n",
    "data = pd.read_csv(read_data)\n",
    "\n",
    "# Initial exploration of data\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "print(\"\\nColumn names and data types in the dataset:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ STEP 1: Drop Unnecessary Columns ------------------\n",
    "data = data.drop(columns=[\n",
    "    'Traffic Density',                # Not needed (regression target)\n",
    "    'Weather',                        # Redundant (Weather_Category kept)\n",
    "    'Date',                           # Already encoded via Day/Hour/Weekend\n",
    "    'Speed_Traffic_Impact'            # Object column; encoded label version exists\n",
    "])\n",
    "\n",
    "# ------------------ STEP 2: Select Features ------------------\n",
    "# Define final feature columns\n",
    "selected_features = [\n",
    "    'City', 'Vehicle Type', 'Weather_Category', 'Economic Condition',\n",
    "    'Day Of Week', 'Hour Of Day', 'Speed', 'Is Peak Hour',\n",
    "    'Random Event Occurred', 'Energy Consumption',\n",
    "    'Is_Public_Holiday', 'Is_Weekend', 'Time of Day',\n",
    "    'Speed_Traffic_Impact_Label'\n",
    "]\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['Traffic Density Category']\n",
    "\n",
    "# Encode the target labels to integers (e.g. 'low' -> 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# ------------------ STEP 3: Preprocessing Pipeline ------------------\n",
    "\n",
    "# Identify categorical columns (if any are still object types)\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ STEP 4: Train-Test-Val Split ------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# ------------------ STEP 5: Fit and Transform ------------------\n",
    "X_train_final_processed = preprocessor.fit_transform(X_train_final)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# ✅ Save the fitted preprocessor\n",
    "joblib.dump(preprocessor, \"C:/Users/Ning Sheng Yong/Desktop/QING APU/traffic_preprocessor.pkl\")\n",
    "\n",
    "# Convert to DataFrame for saving (sparse handling)\n",
    "def to_dataframe(matrix):\n",
    "    return pd.DataFrame(matrix.toarray() if hasattr(matrix, 'toarray') else matrix)\n",
    "\n",
    "X_train_df = to_dataframe(X_train_final_processed)\n",
    "X_val_df = to_dataframe(X_val_processed)\n",
    "X_test_df = to_dataframe(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ STEP 6: Save to CSV ------------------\n",
    "output_path = \"C:/Users/Ning Sheng Yong/Desktop/QING APU/\"\n",
    "\n",
    "X_train_df.to_csv(output_path + \"X_train.csv\", index=False)\n",
    "pd.DataFrame(y_train_final).to_csv(output_path + \"y_train.csv\", index=False)\n",
    "\n",
    "X_val_df.to_csv(output_path + \"X_val.csv\", index=False)\n",
    "pd.DataFrame(y_val).to_csv(output_path + \"y_val.csv\", index=False)\n",
    "\n",
    "X_test_df.to_csv(output_path + \"X_test.csv\", index=False)\n",
    "pd.DataFrame(y_test).to_csv(output_path + \"y_test.csv\", index=False)\n",
    "\n",
    "# ------------------ STATUS ------------------\n",
    "print(\"\\n✅ Datasets saved as CSV:\")\n",
    "print(\"- X_train.csv, y_train.csv\")\n",
    "print(\"- X_val.csv, y_val.csv\")\n",
    "print(\"- X_test.csv, y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b01ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.info())\n",
    "print(pd.Series(y_train).info())\n",
    "print(X_val.info())\n",
    "print(pd.Series(y_val).info())\n",
    "print(X_test.info())\n",
    "print(pd.Series(y_test).info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064bf42",
   "metadata": {},
   "source": [
    "Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5db9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Load Data --------------------\n",
    "X_train = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/X_train.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/y_train.csv\").values.ravel()\n",
    "X_val = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/X_val.csv\")\n",
    "y_val = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/y_val.csv\").values.ravel()\n",
    "X_test = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/X_test.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/Ning Sheng Yong/Desktop/QING APU/y_test.csv\").values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3345c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the target variable for each dataset\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(pd.Series(y_train).value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Validation Set Class Distribution:\")\n",
    "print(pd.Series(y_val).value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Test Set Class Distribution:\")\n",
    "print(pd.Series(y_test).value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11705ac6",
   "metadata": {},
   "source": [
    "Imbalance Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2776fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"Class distribution after SMOTE:\\n\", pd.Series(y_train_smote).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73c9bb",
   "metadata": {},
   "source": [
    "Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Model Evaluation Function ------------------\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_val, y_val, cv=5):\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"\\n==== {model_name} ====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    display(report_df.round(2))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Learning curve\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Accuracy')\n",
    "    plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='Validation Accuracy')\n",
    "\n",
    "    plt.fill_between(train_sizes,\n",
    "                     train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                     train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "                     alpha=0.1)\n",
    "    plt.fill_between(train_sizes,\n",
    "                     val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                     val_scores.mean(axis=1) + val_scores.std(axis=1),\n",
    "                     alpha=0.1)\n",
    "\n",
    "    plt.title(f'Learning Curve - {model_name}')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return acc, report, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76b568",
   "metadata": {},
   "source": [
    "Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2422bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=5000, solver='lbfgs', class_weight='balanced', random_state=42)\n",
    "lr_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "lr_acc, lr_report, lr_cm = evaluate_model(\n",
    "    lr_model,\n",
    "    \"Logistic Regression\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c6b09",
   "metadata": {},
   "source": [
    "Model  2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c594ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "rf_acc, rf_report, rf_cm = evaluate_model(\n",
    "    rf_model,\n",
    "    \"Random Forest\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1418f",
   "metadata": {},
   "source": [
    "Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "xgb_acc, xgb_report, xgb_cm = evaluate_model(\n",
    "    xgb_model,\n",
    "    \"XGBoost\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd46b1",
   "metadata": {},
   "source": [
    "Model 4: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0, random_state=42) \n",
    "cat_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "cat_acc, cat_report, cat_cm = evaluate_model(\n",
    "    cat_model,\n",
    "    \"CatBoost\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")\n",
    "\n",
    "\n",
    "# Export the tuned CatBoost model\n",
    "joblib.dump(cat_model, 'C:/Users/Ning Sheng Yong/Desktop/QING APU/catmodel_traffic_model.pkl')\n",
    "print(\"CatBoost model exported successfully as 'catmodel_traffic_model.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c40490",
   "metadata": {},
   "source": [
    "Model 5: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16698ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "lgbm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "lgbm_acc, lgbm_report, lgbm_cm = evaluate_model( \n",
    "    lgbm_model,\n",
    "    \"LightGBM\",\n",
    "    X_train_smote, y_train_smote, \n",
    "    X_val, y_val\n",
    ")\n",
    "\n",
    "# Export the model\n",
    "joblib.dump(lgbm_model, 'C:/Users/Ning Sheng Yong/Desktop/QING APU/lgbmodel_traffic_model.pkl')\n",
    "print(\"Model exported successfully as 'lgbmodel_traffic_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f3bf4",
   "metadata": {},
   "source": [
    "Model 6: Multi-layer Preception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28645c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "mlp_acc, mlp_report, mlp_cm = evaluate_model(\n",
    "    mlp_model,\n",
    "    \"MLP Classifier\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5bd61",
   "metadata": {},
   "source": [
    "Model 7: K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e053fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "knn_acc, knn_report, knn_cm = evaluate_model(\n",
    "    knn_model,\n",
    "    \"K-Nearest Neighbors\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad816d1d",
   "metadata": {},
   "source": [
    "Model 8: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "gb_acc, gb_report, gb_cm = evaluate_model(\n",
    "    gb_model,\n",
    "    \"Gradient Boosting\",\n",
    "    X_train_smote, y_train_smote,\n",
    "    X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187ba04",
   "metadata": {},
   "source": [
    "Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029bd6b",
   "metadata": {},
   "source": [
    "Tuned Model 1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f859dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'n_estimators': [100, 300, 500]\n",
    "}\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train_smote, y_train_smote, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "evaluate_model(xgb_random.best_estimator_, \"XGBoost (Tuned)\", X_train_smote, y_train_smote, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9fec6",
   "metadata": {},
   "source": [
    "Tuned Model 2: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=1000,           # Increase iterations to allow more rounds, while using early stopping\n",
    "    learning_rate=0.05,        # Lower learning rate for more gradual updates\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    bagging_temperature=1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Use eval_set and early_stopping_rounds for CatBoost tuning\n",
    "cat_model.fit(X_train_smote, y_train_smote, eval_set=(X_val, y_val), verbose=False)\n",
    "evaluate_model(cat_model, \"Tuned CatBoost\", X_train_smote, y_train_smote, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7504a79",
   "metadata": {},
   "source": [
    "Tuned Model 3: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ee55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [10, 20, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "}\n",
    "\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    LGBMClassifier(random_state=42),\n",
    "    param_distributions=lgb_params,\n",
    "    n_iter=50,\n",
    "    cv=5,  # Increased CV folds for robustness\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_random.fit(X_train_smote, y_train_smote, eval_set=[(X_val, y_val)])\n",
    "\n",
    "evaluate_model(lgb_random.best_estimator_, \"LightGBM (Tuned)\", X_train_smote, y_train_smote, X_val, y_val)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
